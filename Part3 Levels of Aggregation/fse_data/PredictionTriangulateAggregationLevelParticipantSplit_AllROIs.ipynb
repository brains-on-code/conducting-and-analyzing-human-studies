{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n",
      "C:\\Users\\author\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\author\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\author\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\author\\Anaconda3\\lib\\importlib\\_bootstrap_external.py:426: ImportWarning: Not importing directory C:\\Users\\author\\Anaconda3\\lib\\site-packages\\mpl_toolkits: missing __init__\n",
      "  _warnings.warn(msg.format(portions[0]), ImportWarning)\n",
      "C:\\Users\\author\\Anaconda3\\lib\\importlib\\_bootstrap_external.py:426: ImportWarning: Not importing directory c:\\users\\author\\anaconda3\\lib\\site-packages\\mpl_toolkits: missing __init__\n",
      "  _warnings.warn(msg.format(portions[0]), ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Read in data\n",
    "z_data = pd.read_csv('fse17_act_deact_zscore_groups.csv',sep=';',decimal='.')\n",
    "\n",
    "#first, drop everything non-numeric ===========================================================\n",
    "z_data = z_data.drop('scan', axis = 1)\n",
    "z_data = z_data.drop('trial', axis = 1)\n",
    "z_data = z_data.drop('response', axis = 1)\n",
    "z_data = z_data.drop('snippet', axis = 1)\n",
    "\n",
    "numLabels = 2 # num of different labels (comprehension and rest)\n",
    "sizeTrainSet = 11 # num of participants used for training\n",
    "sizeTestSet = 3 # num participants used for testing\n",
    "numParticipants = sizeTrainSet + sizeTestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group the data and get the labels after the grouping\n",
    "#return the grouped data and the labels\n",
    "def prepare(data,groupingColumns):\n",
    "    grouped = data.groupby(groupingColumns)\n",
    "    groupedAgg = grouped.aggregate(np.mean)\n",
    "    labels = groupedAgg.index.get_level_values(level='task')\n",
    "    return groupedAgg,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split according to participants (first 12 and last 4)\n",
    "#return all four sets (much like train_test_split from sklearn)\n",
    "#TODO: introduce variation (see file coarseAverageParticipantSplit)\n",
    "\n",
    "def split(features, labels, low, high):\n",
    "    \n",
    "    #=== creating training and validation set ======================================================\n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(features.columns)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    #get the rows excluding the last indexed (i.e., row with the index 587 is the last to include)\n",
    "    training_features = features[0:low]\n",
    "    training_features = np.array(training_features)\n",
    "\n",
    "    testing_features = features[low:high]\n",
    "    testing_features = np.array(testing_features)\n",
    "\n",
    "    training_target = labels[0:low]\n",
    "    training_target = np.array(training_target)\n",
    "    testing_target = labels[low:high]\n",
    "    testing_target = np.array(testing_target)\n",
    "    #=== end creating training and validation set ===================================================\n",
    "    \n",
    "    return training_features, testing_features, training_target, testing_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findIndices(name):\n",
    "    numGroups = int(name.split('_')[1])\n",
    "    low = sizeTrainSet*numGroups*numLabels\n",
    "    high = numParticipants*numGroups*numLabels\n",
    "    return low, high\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "#define the columns to group by\n",
    "groupingColumn1 = 'proband'\n",
    "groupingColumn2 = 'task'\n",
    "groupingColumn3 = ''\n",
    "\n",
    "#define how many different variants of aggregation and aggregation levels\n",
    "numVariationsPerAggLevel = 10\n",
    "numAggLevels = 40\n",
    "numAggregationVariants = numVariationsPerAggLevel * numAggLevels\n",
    "\n",
    "#get the index of the starting column that contains the labels of the groupings\n",
    "offset = z_data.columns.get_loc('aggr_2_groups0')\n",
    "print(offset)\n",
    "collectAccuracies = []\n",
    "\n",
    "#collect all instances where unmatched is not possible.\n",
    "unmatched = []\n",
    "\n",
    "#loop through the columns\n",
    "#for each column, group the data set\n",
    "# transform it to numpy.ndarray and split into training and validation set\n",
    "# run t_pot\n",
    "# and do the learning\n",
    "while offset < len(z_data.columns):\n",
    "    # group the data set\n",
    "#     pdb.set_trace()\n",
    "    #=============================================================================================================\n",
    "    columnsForGrouping = [groupingColumn1,groupingColumn2,z_data.columns[offset]]\n",
    "    features, labels = prepare(z_data,columnsForGrouping)\n",
    "    #=============================================================================================================\n",
    "    #=============================================================================================================\n",
    "    # transform to numpy.ndarray and do split\n",
    "    # find low and high\n",
    "    low, high = findIndices(z_data.columns[offset])\n",
    "    training_features, testing_features, training_target, testing_target = split(features, labels, low, high)\n",
    "    #=============================================================================================================\n",
    "    print(offset)\n",
    "    print(low, high)\n",
    "    \n",
    "    folder = 'AllROIs/'\n",
    "    fileName = 'tpot_mnist_pipeline_triangulateAggregationLevelParticipantSplit' + z_data.columns[offset] + '.py'\n",
    "    #=============================================================================================================\n",
    "    # run t_pot\n",
    "    # TODO: extract method\n",
    "    tpot = TPOTClassifier(generations=5, population_size=5, verbosity=2, n_jobs=20,max_eval_time_mins=1)\n",
    "    tpot.fit(training_features, training_target)\n",
    "    print(tpot.score(testing_features, testing_target))\n",
    "    tpot.export(folder + fileName)\n",
    "#     #=============================================================================================================\n",
    "    \n",
    "    offset = offset + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "392\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "393\n",
      "44 56\n",
      "Number of matches: 7 (of 12 )\n",
      "Accuary:  58.333333333333336\n",
      "394\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "395\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "396\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "397\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "398\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "399\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "400\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "401\n",
      "44 56\n",
      "Number of matches: 12 (of 12 )\n",
      "Accuary:  100.0\n",
      "402\n",
      "66 84\n",
      "Number of matches: 17 (of 18 )\n",
      "Accuary:  94.44444444444444\n",
      "403\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "404\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "405\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "406\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "407\n",
      "66 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\author\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 17 (of 18 )\n",
      "Accuary:  94.44444444444444\n",
      "408\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "409\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "410\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "411\n",
      "66 84\n",
      "Number of matches: 18 (of 18 )\n",
      "Accuary:  100.0\n",
      "412\n",
      "198 252\n",
      "Number of matches: 49 (of 54 )\n",
      "Accuary:  90.74074074074075\n",
      "413\n",
      "198 252\n",
      "Number of matches: 48 (of 54 )\n",
      "Accuary:  88.88888888888889\n",
      "414\n",
      "198 252\n",
      "Number of matches: 52 (of 54 )\n",
      "Accuary:  96.29629629629629\n",
      "415\n",
      "198 252\n",
      "Number of matches: 50 (of 54 )\n",
      "Accuary:  92.5925925925926\n",
      "416\n",
      "198 252\n",
      "Number of matches: 50 (of 54 )\n",
      "Accuary:  92.5925925925926\n",
      "417\n",
      "198 252\n",
      "Number of matches: 52 (of 54 )\n",
      "Accuary:  96.29629629629629\n",
      "418\n",
      "198 252\n",
      "Number of matches: 53 (of 54 )\n",
      "Accuary:  98.14814814814815\n",
      "419\n",
      "198 252\n",
      "Number of matches: 51 (of 54 )\n",
      "Accuary:  94.44444444444444\n",
      "420\n",
      "198 252\n",
      "Number of matches: 51 (of 54 )\n",
      "Accuary:  94.44444444444444\n",
      "421\n",
      "198 252\n",
      "Number of matches: 48 (of 54 )\n",
      "Accuary:  88.88888888888889\n",
      "422\n",
      "286 364\n",
      "Number of matches: 71 (of 78 )\n",
      "Accuary:  91.02564102564102\n",
      "423\n",
      "286 364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 70 (of 78 )\n",
      "Accuary:  89.74358974358975\n",
      "424\n",
      "286 364\n",
      "Number of matches: 70 (of 78 )\n",
      "Accuary:  89.74358974358975\n",
      "425\n",
      "286 364\n",
      "Number of matches: 69 (of 78 )\n",
      "Accuary:  88.46153846153845\n",
      "426\n",
      "286 364\n",
      "Number of matches: 70 (of 78 )\n",
      "Accuary:  89.74358974358975\n",
      "427\n",
      "286 364\n",
      "Number of matches: 68 (of 78 )\n",
      "Accuary:  87.17948717948718\n",
      "428\n",
      "286 364\n",
      "Number of matches: 74 (of 78 )\n",
      "Accuary:  94.87179487179486\n",
      "429\n",
      "286 364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\author\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 71 (of 78 )\n",
      "Accuary:  91.02564102564102\n",
      "430\n",
      "286 364\n",
      "Number of matches: 73 (of 78 )\n",
      "Accuary:  93.58974358974359\n",
      "431\n",
      "286 364\n",
      "Number of matches: 73 (of 78 )\n",
      "Accuary:  93.58974358974359\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\fMRIML\\FSE_full\\AllROIs\\cleaned_tpot_mnist_pipeline_triangulateAggregationLevelParticipantSplitaggr_13_groups9.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# transform to numpy.ndarray and do split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# find low and high\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindIndices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mtraining_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m#=============================================================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\fMRIML\\FSE_full\\AllROIs\\cleaned_tpot_mnist_pipeline_triangulateAggregationLevelParticipantSplitaggr_13_groups9.py\u001b[0m in \u001b[0;36mfindIndices\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfindIndices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mnumGroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msizeTrainSet\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnumGroups\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnumLabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mhigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumParticipants\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnumGroups\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnumLabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#get the index of the starting column that contains the labels of the groupings\n",
    "offset = z_data.columns.get_loc('aggr_2_groups0')\n",
    "print(offset)\n",
    "collectAccuracies = []\n",
    "\n",
    "#collect all instances where unmatched is not possible.\n",
    "unmatched = []\n",
    "\n",
    "#loop through the columns\n",
    "#for each column, group the data set\n",
    "# transform it to numpy.ndarray and split into training and validation set\n",
    "# run t_pot\n",
    "# and do the learning\n",
    "while offset < len(z_data.columns):    \n",
    "\n",
    "    #=============================================================================================================\n",
    "    columnsForGrouping = ['proband','task',z_data.columns[offset]]\n",
    "    features, labels = prepare(z_data,columnsForGrouping)\n",
    "    #=============================================================================================================\n",
    "    #=============================================================================================================\n",
    "    # transform to numpy.ndarray and do split\n",
    "    # find low and high\n",
    "    low, high = findIndices(z_data.columns[offset])\n",
    "    training_features, testing_features, training_target, testing_target = split(features, labels, low, high)\n",
    "    #=============================================================================================================\n",
    "    \n",
    "    fileName = 'tpot_mnist_pipeline_triangulateAggregationLevelParticipantSplit' + z_data.columns[offset] + '.py'\n",
    "\n",
    "    #=============================================================================================================\n",
    "    #do the learning\n",
    "    # TODO: extract method\n",
    "    with open(folder + fileName) as f:\n",
    "        content = f.readlines()\n",
    "    cleanedContent = []\n",
    "    for line in content:\n",
    "        if 'tpot_data' not in line and 'training_target, testing_target' not in line:\n",
    "            cleanedContent.append(line)\n",
    "    \n",
    "    fileNameCleaned = folder + 'cleaned_' + fileName\n",
    "    with open(fileNameCleaned, 'w') as filehandle:  \n",
    "        for line in cleanedContent:\n",
    "            filehandle.write('%s\\n' % line)\n",
    "            \n",
    "    %run -i $fileNameCleaned\n",
    "    \n",
    "    # print the accuracy\n",
    "    # TODO: extract method\n",
    "    # TODO: print all accurracy values to one file\n",
    "    num_matches = 0;\n",
    "    for a, b in zip(testing_target, results):\n",
    "        if a == b:\n",
    "            num_matches = num_matches + 1\n",
    "        else:\n",
    "            unmatched.append((a,b))\n",
    "    print('Number of matches:',num_matches,'(of',testing_target.size,')')\n",
    "\n",
    "    accuracy = num_matches/testing_target.size*100\n",
    "    print('Accuary: ',accuracy)\n",
    "    #=============================================================================================================\n",
    "    \n",
    "    #=============================================================================================================\n",
    "    #collect all accuracy values to plot them\n",
    "    collectAccuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "    offset = offset + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehension/Rest 114\n",
      "Comprehension/Syntax 0\n",
      "Rest/Syntax 0\n"
     ]
    }
   ],
   "source": [
    "countComprehensionRest = 0\n",
    "countComprehensionSyntax = 0\n",
    "countRestSyntax = 0\n",
    "for x in unmatched:\n",
    "    if x == (0,1) or x == (1,0):\n",
    "        countComprehensionRest = countComprehensionRest + 1\n",
    "    if x == (0,2) or x == (2,0):\n",
    "        countComprehensionSyntax = countComprehensionSyntax + 1\n",
    "    if x == (1,2) or x == (2,1):\n",
    "        countRestSyntax = countRestSyntax + 1\n",
    "print('Comprehension/Rest',countComprehensionRest)\n",
    "print('Comprehension/Syntax',countComprehensionSyntax)\n",
    "print('Rest/Syntax',countRestSyntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(alist, wanted_parts=1):\n",
    "    length = len(alist)\n",
    "    return [ alist[i*length // wanted_parts: (i+1)*length // wanted_parts] \n",
    "             for i in range(wanted_parts) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1, list2, list3, list4 = split_list(collectAccuracies,4)\n",
    "frames = [list1, list2, list3, list4]\n",
    "collectedAccuracies = pd.DataFrame(frames).T\n",
    "collectedAccuracies.columns = ['2_groups_allROIs','3_groups_allROIs','9_groups_allROIs','13_groups_allROIs']\n",
    "\n",
    "\n",
    "collectedAccuracies.to_csv('triangulate_allROIs.csv',sep=';',decimal='.', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
